{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd0bc9b77aa262136fbbe20e9862d9cca86a592e78b9b4bad3343e4f88122d1f6c1",
   "display_name": "Python 3.6.13 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Python Name Disambiguation Metrics (ND_Metrics)\n",
    "\n",
    "Metrics implementation in Python for Author Name Disambiguation (AND) evaluation methods.\n",
    "\n",
    "More information about metrics and details of the algorithms can be found in the following publications:\n",
    "\n",
    "> Kim, J. \"A fast and integrative algorithm for clustering performance evaluation in author name disambiguation.\" Scientometrics (2019): 661-681, 120(2).\n",
    "\n",
    "> Amigó, Enrique, et al.: A comparison of Extrinsic Clustering Evaluation Metrics based on Formal Constraints. In: Information Retrieval 12.4 (2009): 461-486.\n",
    "\n",
    "\n",
    "## Installation (not yet implemented)\n",
    "\n",
    "You can simply use `pip` (or any similar package manager) for installation:\n",
    "\n",
    "    pip install python-nd_metrics\n",
    "\n",
    "or, if you prefer a local user installation:\n",
    "\n",
    "    pip install --user python-nd_metrics\n",
    "\n",
    "## Usage\n",
    "\n",
    "To evaluate any algorithm output you will need **ground-truth data** (also called gold-standard data). We call this the `y_true`. The ground-truth is represented in a:\n",
    "\n",
    "- (1) dictionary, where the keys are author labels in the gold-standard and the values are sets of annotated categories for those auhor lables. Or; \n",
    "- (2) list/numpy 1-d array, where author labels in the gold-standard as list or numpy 1-d array. In this case, the index vector represents the citations of each author.\n",
    "\n",
    "\n",
    "For examples:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# For (1) representation\n",
    "y_true: {\n",
    "    1: set({0, 1, 2}), \n",
    "    2: set({3, 4}), \n",
    "    3: set({5, 6, 7})\n",
    "}\n",
    "\n",
    "# For (2) representation\n",
    "y_true = np.array([1,1,1,2,2,3,3,3])\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "In the above example, key `1` on (1) `y_true` is an author label `1`, with contains three citações in the ground-truth. The same applies to the other keys (`2` and `3`).\n",
    "\n",
    "\n",
    "The **algorithm output** to be evaluated is called the `y_pred` and is also represented as same cases of `y_true` (ground-truth) \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# For (1) representation\n",
    "y_pred: {\n",
    "     1: set({0, 1, 2}), \n",
    "     2: set({3, 4, 5, 6, 7})\n",
    "}\n",
    "\n",
    "# For (2) representation\n",
    "y_pred = np.array([1,1,1,2,2,2,2,2])\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Please note that the author names (or keys in dict) **do not need** to be the same as in the ground-truth data because the algorithm only considers the groupings, it does not try to match the names of clusters to the ground-truth labels.\n",
    "\n",
    "\n",
    "Once you have defined the `y_true` (ground-truth data) and the `y_pred` (algorithm output to evaluate), you can simply do the following to obtain metric values:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "from nd_metrics.utils import make_clusters\n",
    "from nd_metrics.metrics import all_metrics\n",
    "\n",
    "# true and pred labels as list or numpy 1-d array\n",
    "y_true = np.array([1,1,1,2,2,3,3,3])\n",
    "y_pred = np.array([1,1,1,2,2,2,2,2])\n",
    "\n",
    "# true and pred clusters as dict of sets\n",
    "y_true_c, y_pred_c = make_clusters(y_true, y_pred)\n",
    "# y_true_c: {1: set({0, 1, 2}), 2: set({3, 4}), 3: set({5, 6, 7})}\n",
    "# y_pred_c: {1: set({0, 1, 2}), 2: set({3, 4, 5, 6, 7})}\n",
    "\n",
    "# all_metrics calculate\n",
    "all_metrics(y_true_c, y_pred_c)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "A simple example can be found in the included `example.py` file, where the examples of the source publication are used.\n",
    "\n",
    "## License\n",
    "\n",
    "This software is under the **Apache License 2.0**.\n",
    "\n",
    "    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "    you may not use this file except in compliance with the License.\n",
    "    You may obtain a copy of the License at\n",
    "\n",
    "        http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing, software\n",
    "    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "    See the License for the specific language governing permissions and\n",
    "    limitations under the License.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example import test_all_metrics_classes, test_all_metrics_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ClusterFMetric': (0.5, 0.3333333333333333, 0.3999999520000058),\n",
       " 'KMetric': (0.7, 1.0, 0.8366600265340756),\n",
       " 'BCubedMetric': (0.7, 1.0, 0.823529363321802),\n",
       " 'SELEMetric': (0.0, 0.38461538461538464, 0.0),\n",
       " 'PairwiseFMetric': (0.5384615384615384, 1.0, 0.699999954500003)}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "test_all_metrics_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ClusterFMetric': (0.5, 0.3333333333333333, 0.3999999520000058),\n",
       " 'KMetric': (0.7, 1.0, 0.8366600265340756),\n",
       " 'BCubedMetric': (0.7, 1.0, 0.823529363321802),\n",
       " 'PairwiseFMetric': (0.5384615384615384, 1.0, 0.699999954500003)}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "test_all_metrics_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nd_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('nd_metrics', '0.0.1')"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "nd_metrics.name, nd_metrics.__version__"
   ]
  }
 ]
}